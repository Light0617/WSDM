{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "import math\n",
    "import gc\n",
    "import pickle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "train = pickle.load( open(data_path + 'train_feature_myadding1_reduceDim', 'rb'))\n",
    "#train = pickle.load( open(data_path + 'train_feature_myadding', 'rb'))\n",
    "print 'loading done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "#test = pickle.load( open(data_path + 'test_feature_myadding', 'rb'))\n",
    "test = pickle.load( open(data_path + 'test_feature_myadding1_reduceDim', 'rb'))\n",
    "print 'loading done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songs = pd.read_csv(data_path + 'songs1.csv')\n",
    "# song_cols = ['song_id', 'song_country', 'song_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = test.merge(songs[song_cols], on='song_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.merge(songs[song_cols], on='song_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msno                        category\n",
      "song_id                        int64\n",
      "source_system_tab           category\n",
      "source_screen_name          category\n",
      "source_type                 category\n",
      "target                         uint8\n",
      "song_length                   uint32\n",
      "genre_ids                   category\n",
      "artist_name                 category\n",
      "composer                    category\n",
      "lyricist                    category\n",
      "language                    category\n",
      "city                        category\n",
      "bd                             uint8\n",
      "gender                      category\n",
      "registered_via              category\n",
      "expiration_date                int64\n",
      "registration_year              int64\n",
      "registration_month             int64\n",
      "registration_date              int64\n",
      "expiration_year                int64\n",
      "expiration_month               int64\n",
      "membership_days                int64\n",
      "genre_ids_count                 int8\n",
      "lyricists_count                 int8\n",
      "composer_count                  int8\n",
      "is_featured                     int8\n",
      "artist_count                    int8\n",
      "artist_composer                 int8\n",
      "artist_composer_lyricist        int8\n",
      "song_lang_boolean               int8\n",
      "smaller_song                    int8\n",
      "count_song_played              int64\n",
      "count_artist_played            int64\n",
      "song_country                category\n",
      "song_year_y                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(train, open(data_path + 'train_feature_myadding', 'wb'))\n",
    "# pickle.dump(test, open(data_path + 'test_feature_myadding', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train test and validation sets\n",
      "Processed data...\n"
     ]
    }
   ],
   "source": [
    "print (\"Train test and validation sets\")\n",
    "for col in train.columns:\n",
    "    if train[col].dtype == object:\n",
    "        train[col] = train[col].astype('category')\n",
    "        test[col] = test[col].astype('category')\n",
    "\n",
    "\n",
    "X_train = train.drop(['target'], axis=1)\n",
    "y_train = train['target'].values\n",
    "\n",
    "\n",
    "X_test = test.drop(['id'], axis=1)\n",
    "ids = test['id'].values\n",
    "\n",
    "length = int(len(X_train)*0.8)\n",
    "X_trainSet = X_train[:length]\n",
    "y_trainSet = y_train[:length]\n",
    "X_valSet = X_train[length:]\n",
    "y_valSet =  y_train[length:]\n",
    "# del train, test; gc.collect();\n",
    "\n",
    "# X_trainSet = X_train\n",
    "# y_trainSet = y_train\n",
    "# X_valSet = X_train\n",
    "# y_valSet =  y_train\n",
    "\n",
    "d_train_final = lgb.Dataset(X_trainSet, y_trainSet)\n",
    "watchlist_final = lgb.Dataset(X_valSet, y_valSet)\n",
    "train_final = lgb.Dataset(X_train, y_train)\n",
    "print('Processed data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1249054  428911  105604  517911 1330201  508297  972296 1052122  522545\n",
      "  167428]\n",
      "1475484\n",
      "[1 1 0 0 0 1 0 0 1 1]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "sample_size = 10\n",
    "sample_index = np.random.choice(range(len(X_valSet)) ,size = sample_size, replace=False)\n",
    "print sample_index \n",
    "print len(y_valSet)\n",
    "real_y =  y_valSet[sample_index]\n",
    "print real_y\n",
    "fpr, tpr, thresholds =  metrics.roc_curve(real_y, [0] *10)\n",
    "print metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 5\n",
    "vals = []\n",
    "for i in range(fold):\n",
    "    sample_size = len(X_valSet) * (fold - 1) / fold\n",
    "    sample_index = np.random.choice(range(len(X_valSet)) ,size = sample_size, replace=False)\n",
    "    valSet = lgb.Dataset(X_valSet.iloc[sample_index], y_valSet[sample_index])\n",
    "    real_y = y_valSet[sample_index]\n",
    "    vals += lgb.Dataset(X_valSet.iloc[sample_index], y_valSet[sample_index]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(max_depth = 16, tree = 'gbdt', fold = 5, max_bin = 50, bagging_fraction = 0.95 ):\n",
    "    n_round = 200 + (15 - max_depth) * 25\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': tree,\n",
    "        'learning_rate': 0.6,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 108,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': max_bin,\n",
    "        'max_depth': max_depth,\n",
    "        'num_rounds': 200,\n",
    "        'metric' : 'auc'\n",
    "    }\n",
    "    frac = 1 - (1.0 / fold)\n",
    "    evals_result = {}\n",
    "    model_f = lgb.train(params, train_set=d_train_final,  valid_sets=watchlist_final, verbose_eval=200,\\\n",
    "                              evals_result=evals_result)\n",
    "    print 'done'\n",
    "    p_test = model_f.predict(X_test)\n",
    "    print  'mean=', evals_result['valid_0']['auc'][-1]\n",
    "    return [p_test, evals_result['valid_0']['auc'][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model_final(max_depth = 16, tree = 'gbdt', fold = 5, max_bin = 50, bagging_fraction = 0.95 ):\n",
    "    n_round = 200 + (max_depth - 15) * 5\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'boosting': tree,\n",
    "        'learning_rate': 0.6,\n",
    "        'verbose': 0,\n",
    "        'num_leaves': 108,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'bagging_freq': 1,\n",
    "        'bagging_seed': 1,\n",
    "        'feature_fraction': 0.9,\n",
    "        'feature_fraction_seed': 1,\n",
    "        'max_bin': max_bin,\n",
    "        'max_depth': max_depth,\n",
    "        'num_rounds': 200,\n",
    "        'metric' : 'auc'\n",
    "    }\n",
    "    evals_result = {}\n",
    "    model_f = lgb.train(params, train_set=train_final,  valid_sets=train_final, verbose_eval=200,\\\n",
    "                              evals_result=evals_result)\n",
    "    print 'done'\n",
    "    p_test = model_f.predict(X_test)\n",
    "    print  'mean=', evals_result['training']['auc'][-1]\n",
    "    return [p_test, evals_result['training']['auc'][-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dart\n",
      "max_depth... 22\n",
      "[200]\tvalid_0's auc: 0.683127\n",
      "done\n",
      "mean= 0.683126554982\n",
      "max_depth... 23\n",
      "[200]\tvalid_0's auc: 0.684025\n",
      "done\n",
      "mean= 0.684025200776\n",
      "max_depth... 24\n",
      "[200]\tvalid_0's auc: 0.683424\n",
      "done\n",
      "mean= 0.683423870829\n",
      "max_depth... 25\n",
      "[200]\tvalid_0's auc: 0.683243\n",
      "done\n",
      "mean= 0.683242848975\n",
      "max_depth... 26\n",
      "[200]\tvalid_0's auc: 0.684003\n",
      "done\n",
      "mean= 0.684003282544\n",
      "max_depth... 27\n",
      "[200]\tvalid_0's auc: 0.683639\n",
      "done\n",
      "mean= 0.683639398343\n",
      "max_depth... 28\n",
      "[200]\tvalid_0's auc: 0.683466\n",
      "done\n",
      "mean= 0.68346612941\n",
      "max_depth... 29\n",
      "[200]\tvalid_0's auc: 0.683804\n",
      "done\n",
      "mean= 0.683804281482\n",
      "max_depth... 30\n",
      "[200]\tvalid_0's auc: 0.684491\n",
      "done\n",
      "mean= 0.684490583375\n",
      "gbdt\n",
      "max_depth... 22\n",
      "[200]\tvalid_0's auc: 0.67788\n",
      "done\n",
      "mean= 0.677880199916\n",
      "max_depth... 23\n",
      "[200]\tvalid_0's auc: 0.676867\n",
      "done\n",
      "mean= 0.676866752141\n",
      "max_depth... 24\n",
      "[200]\tvalid_0's auc: 0.677996\n",
      "done\n",
      "mean= 0.677996261719\n",
      "max_depth... 25\n",
      "[200]\tvalid_0's auc: 0.677539\n",
      "done\n",
      "mean= 0.677538786505\n",
      "max_depth... 26\n",
      "[200]\tvalid_0's auc: 0.678415\n",
      "done\n",
      "mean= 0.678414907137\n",
      "max_depth... 27\n",
      "[200]\tvalid_0's auc: 0.678192\n",
      "done\n",
      "mean= 0.678191940942\n",
      "max_depth... 28\n",
      "[200]\tvalid_0's auc: 0.677959\n",
      "done\n",
      "mean= 0.677958934205\n",
      "max_depth... 29\n",
      "[200]\tvalid_0's auc: 0.67775\n",
      "done\n",
      "mean= 0.677749929258\n",
      "max_depth... 30\n",
      "[200]\tvalid_0's auc: 0.678718\n",
      "done\n",
      "mean= 0.678718033285\n"
     ]
    }
   ],
   "source": [
    "weights, p_tests = [], []\n",
    "max_bin = 50\n",
    "for tree in ['dart', 'gbdt']:\n",
    "    print tree\n",
    "    for max_depth in range(22,31,1):\n",
    "        print 'max_depth...', max_depth\n",
    "        p_test, res = lgb_model(tree = tree, max_depth = max_depth)\n",
    "        #p_test, res= lgb_model_final(tree = tree, max_depth = max_depth)\n",
    "        weights += res,\n",
    "        p_tests += p_test,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dart\n",
      "max_depth... 28\n",
      "[200]\ttraining's auc: 0.837109\n",
      "done\n",
      "mean= 0.837109407473\n",
      "max_depth... 29\n",
      "[200]\ttraining's auc: 0.837426\n",
      "done\n",
      "mean= 0.837426078896\n",
      "max_depth... 30\n",
      "[200]\ttraining's auc: 0.837214\n",
      "done\n",
      "mean= 0.837213799781\n"
     ]
    }
   ],
   "source": [
    "weights, p_tests = [], []\n",
    "max_bin = 50\n",
    "for tree in ['dart']:\n",
    "    print tree\n",
    "    for max_depth in range(28,31,1):\n",
    "        print 'max_depth...', max_depth\n",
    "        p_test, res= lgb_model_final(tree = tree, max_depth = max_depth)\n",
    "        weights += res,\n",
    "        p_tests += p_test,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = [(x * 10) ** 2 for x in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print len(weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(p_tests, open(data_path + 'p_tests', 'wb'))\n",
    "# pickle.dump(weights1, open(data_path + 'weights', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done making predictions\n"
     ]
    }
   ],
   "source": [
    "result = (np.array(p_tests).T * np.array(weights)).T\n",
    "print('Done making predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2556790)\n",
      "(2556790,)\n",
      "0.391422638034\n",
      "0.372698401874\n",
      "0.17617924762\n"
     ]
    }
   ],
   "source": [
    "print (np.shape(result))\n",
    "total = np.sum(weights)\n",
    "mat = np.sum(np.matrix(result), axis = 0)\n",
    "p_test = [x / total for x in (mat.tolist()[0])]\n",
    "print (np.shape(p_test))\n",
    "for x in p_test[:3]:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions Model model of gbdt\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print ('Saving predictions Model model of gbdt')\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = ids\n",
    "subm['target'] = p_test\n",
    "subm.to_csv(data_path + '../submissions/submission_lgbm_27_30.csv.gz', compression = 'gzip', index=False, float_format = '%.5f')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dart\n",
    "# max_depth... 22\n",
    "# [200]\tvalid_0's auc: 0.683127\n",
    "# done\n",
    "# mean= 0.683126554982\n",
    "# max_depth... 23\n",
    "# [200]\tvalid_0's auc: 0.684025\n",
    "# done\n",
    "# mean= 0.684025200776\n",
    "# max_depth... 24\n",
    "# [200]\tvalid_0's auc: 0.683424\n",
    "# done\n",
    "# mean= 0.683423870829\n",
    "# max_depth... 25\n",
    "# [200]\tvalid_0's auc: 0.683243\n",
    "# done\n",
    "# mean= 0.683242848975\n",
    "# max_depth... 26\n",
    "# [200]\tvalid_0's auc: 0.684003\n",
    "# done\n",
    "# mean= 0.684003282544\n",
    "# max_depth... 27\n",
    "# [200]\tvalid_0's auc: 0.683639\n",
    "# done\n",
    "# mean= 0.683639398343\n",
    "# max_depth... 28\n",
    "# [200]\tvalid_0's auc: 0.683466\n",
    "# done\n",
    "# mean= 0.68346612941\n",
    "# max_depth... 29\n",
    "# [200]\tvalid_0's auc: 0.683804\n",
    "# done\n",
    "# mean= 0.683804281482\n",
    "# max_depth... 30\n",
    "# [200]\tvalid_0's auc: 0.684491\n",
    "# done\n",
    "# mean= 0.684490583375\n",
    "# gbdt\n",
    "# max_depth... 22\n",
    "# [200]\tvalid_0's auc: 0.67788\n",
    "# done\n",
    "# mean= 0.677880199916\n",
    "# max_depth... 23\n",
    "# [200]\tvalid_0's auc: 0.676867\n",
    "# done\n",
    "# mean= 0.676866752141\n",
    "# max_depth... 24\n",
    "# [200]\tvalid_0's auc: 0.677996\n",
    "# done\n",
    "# mean= 0.677996261719\n",
    "# max_depth... 25\n",
    "# [200]\tvalid_0's auc: 0.677539\n",
    "# done\n",
    "# mean= 0.677538786505\n",
    "# max_depth... 26\n",
    "# [200]\tvalid_0's auc: 0.678415\n",
    "# done\n",
    "# mean= 0.678414907137\n",
    "# max_depth... 27\n",
    "# [200]\tvalid_0's auc: 0.678192\n",
    "# done\n",
    "# mean= 0.678191940942\n",
    "# max_depth... 28\n",
    "# [200]\tvalid_0's auc: 0.677959\n",
    "# done\n",
    "# mean= 0.677958934205\n",
    "# max_depth... 29\n",
    "# [200]\tvalid_0's auc: 0.67775\n",
    "# done\n",
    "# mean= 0.677749929258\n",
    "# max_depth... 30\n",
    "# [200]\tvalid_0's auc: 0.678718\n",
    "# done\n",
    "# mean= 0.678718033285"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
